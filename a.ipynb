{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1eb6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\conda.exe\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\conda-env.exe\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\conda-script.py\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\conda-env-script.py\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\conda.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Library\\bin\\conda.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\_conda_activate.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\rename_tmp.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\conda_auto_activate.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\conda_hook.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\activate.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\activate.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\condabin\\deactivate.bat\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\activate\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Scripts\\deactivate\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\etc\\profile.d\\conda.sh\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\etc\\fish\\conf.d\\conda.fish\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\shell\\condabin\\Conda.psm1\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\shell\\condabin\\conda-hook.ps1\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\Lib\\site-packages\\xontrib\\conda.xsh\n",
      "no change     C:\\Users\\Paulo\\miniconda3\\etc\\profile.d\\conda.csh\n",
      "no change     C:\\Users\\Paulo\\Documents\\WindowsPowerShell\\profile.ps1\n",
      "No action taken.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda init powershell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a20299",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name myenv python=3.12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3989e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate myenv\n",
    "!pip install ir_datasets datasets sentence-transformers scikit-learn pandas tqdm ir_datasets faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed3abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\paulo\\miniconda3\\lib\\site-packages (4.67.1)\n",
      "Collecting ir_datasets\n",
      "  Using cached ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.11.0.post1-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\paulo\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (24.2)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.14-cp313-cp313-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.7.1-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paulo\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\paulo\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting beautifulsoup4>=4.4.1 (from ir_datasets)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
      "  Using cached inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting lxml>=4.5.2 (from ir_datasets)\n",
      "  Using cached lxml-6.0.0-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
      "  Using cached trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir_datasets)\n",
      "  Downloading lz4-4.4.4-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
      "  Using cached warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
      "  Using cached warc3_wet_clueweb09-0.2.5-py3-none-any.whl\n",
      "Collecting zlib-state>=0.1.3 (from ir_datasets)\n",
      "  Using cached zlib_state-0.1.9.tar.gz (9.5 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ijson>=3.1.3 (from ir_datasets)\n",
      "  Using cached ijson-3.4.0-cp313-cp313-win_amd64.whl.metadata (22 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
      "  Using cached unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->ir_datasets)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paulo\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\paulo\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
      "  Using cached cbor-1.0.0-py3-none-any.whl\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 6.8/10.8 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 36.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 44.6 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  8.7/8.7 MB 43.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 40.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.1-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------  10.7/11.0 MB 60.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 27.1 MB/s eta 0:00:00\n",
      "Using cached ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
      "Using cached faiss_cpu-1.11.0.post1-cp313-cp313-win_amd64.whl (14.9 MB)\n",
      "Downloading numpy-2.3.1-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.6/12.7 MB 61.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 55.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.12.14-cp313-cp313-win_amd64.whl (448 kB)\n",
      "Downloading multidict-6.6.3-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached ijson-3.4.0-cp313-cp313-win_amd64.whl (54 kB)\n",
      "Using cached inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached lxml-6.0.0-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "Downloading lz4-4.4.4-cp313-cp313-win_amd64.whl (99 kB)\n",
      "Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl (26.1 MB)\n",
      "   ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 13.6/26.1 MB 65.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.1 MB 62.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.1/26.1 MB 57.1 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading scipy-1.16.0-cp313-cp313-win_amd64.whl (38.4 MB)\n",
      "   ---------------------------------------- 0.0/38.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 13.6/38.4 MB 64.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.5/38.4 MB 65.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.4/38.4 MB 59.6 MB/s eta 0:00:00\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading torch-2.7.1-cp313-cp313-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 12.8/216.1 MB 63.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 26.2/216.1 MB 63.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 40.1/216.1 MB 63.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 54.0/216.1 MB 64.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 68.4/216.1 MB 64.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 82.3/216.1 MB 64.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 92.8/216.1 MB 62.3 MB/s eta 0:00:02\n",
      "   ------------------ -------------------- 105.1/216.1 MB 61.7 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 119.0/216.1 MB 62.1 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 132.9/216.1 MB 62.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 146.0/216.1 MB 62.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 159.9/216.1 MB 62.3 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 173.8/216.1 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 185.1/216.1 MB 61.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 199.0/216.1 MB 62.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  211.8/216.1 MB 62.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 62.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 57.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 55.9 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
      "Using cached warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 52.3 MB/s eta 0:00:00\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 7.0/7.0 MB 59.4 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Building wheels for collected packages: zlib-state\n",
      "  Building wheel for zlib-state (pyproject.toml): started\n",
      "  Building wheel for zlib-state (pyproject.toml): finished with status 'error'\n",
      "Failed to build zlib-state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for zlib-state (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [32 lines of output]\n",
      "      C:\\Users\\Paulo\\AppData\\Local\\Temp\\pip-build-env-4gulcf17\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: MIT License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-313\\zlib_state\n",
      "      copying zlib_state\\__init__.py -> build\\lib.win-amd64-cpython-313\\zlib_state\n",
      "      running egg_info\n",
      "      writing zlib_state.egg-info\\PKG-INFO\n",
      "      writing dependency_links to zlib_state.egg-info\\dependency_links.txt\n",
      "      writing top-level names to zlib_state.egg-info\\top_level.txt\n",
      "      reading manifest file 'zlib_state.egg-info\\SOURCES.txt'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'zlib_state.egg-info\\SOURCES.txt'\n",
      "      running build_ext\n",
      "      building '_zlib_state' extension\n",
      "      creating build\\temp.win-amd64-cpython-313\\Release\\src\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD \"-IC:/Program Files/zlib\\include\" -IC:\\Users\\Paulo\\miniconda3\\include -IC:\\Users\\Paulo\\miniconda3\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\" /Tcsrc/zlib_state.c /Fobuild\\temp.win-amd64-cpython-313\\Release\\src\\zlib_state.obj\n",
      "      zlib_state.c\n",
      "      src/zlib_state.c(7): fatal error C1083: N\\xc6o \\x82 poss\\xa1vel abrir arquivo incluir: 'C:/Program Files/zlib/include/zlib.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for zlib-state\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (zlib-state)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sentence-transformers scikit-learn pandas tqdm ir_datasets faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cdda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 04:04:05,073 - INFO - ================================================================================\n",
      "2025-07-20 04:04:05,074 - INFO - STARTING FULL MS MARCO EVALUATION SYSTEM\n",
      "2025-07-20 04:04:05,075 - INFO - This will download and process the COMPLETE MS MARCO dataset\n",
      "2025-07-20 04:04:05,075 - INFO - ================================================================================\n",
      "2025-07-20 04:04:05,077 - INFO - Initializing MS MARCO Evaluator with save directory: ./msmarco_full_eval\n",
      "2025-07-20 04:04:05,078 - INFO - Loading sentence transformer model: all-MiniLM-L6-v2\n",
      "2025-07-20 04:04:05,086 - INFO - Use pytorch device_name: cpu\n",
      "2025-07-20 04:04:05,087 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-07-20 04:04:08,744 - INFO - Model loaded successfully. Max sequence length: 256\n",
      "2025-07-20 04:04:08,745 - INFO - Using CPU for inference\n",
      "2025-07-20 04:04:08,745 - INFO - STEP 1: Loading complete MS MARCO dataset...\n",
      "2025-07-20 04:04:08,745 - INFO - Loading FULL MS MARCO dataset...\n",
      "2025-07-20 04:04:08,746 - INFO - ================================================================================\n",
      "2025-07-20 04:04:08,747 - INFO - LOADING MS MARCO PASSAGE RANKING DATASET - FULL VERSION\n",
      "2025-07-20 04:04:08,747 - INFO - This will download and process the complete dataset\n",
      "2025-07-20 04:04:08,747 - INFO - ================================================================================\n",
      "2025-07-20 04:04:08,748 - INFO - Loading MS MARCO collection using HuggingFace datasets...\n",
      "2025-07-20 04:04:08,748 - INFO - Step 1: Loading passage collection...\n",
      "2025-07-20 04:04:09,187 - ERROR - Error loading collection via HuggingFace: Dataset 'microsoft/msmarco' doesn't exist on the Hub or cannot be accessed.\n",
      "2025-07-20 04:04:09,188 - INFO - Falling back to ir_datasets...\n",
      "2025-07-20 04:04:09,189 - INFO - Loading with ir_datasets as fallback...\n",
      "2025-07-20 04:04:09,190 - INFO - Trying to load dataset: msmarco-passage\n",
      "2025-07-20 04:04:09,191 - INFO - Successfully loaded: msmarco-passage\n",
      "2025-07-20 04:04:09,191 - INFO - Loading documents...\n",
      "Loading documents: 92935it [00:01, 71940.48it/s]2025-07-20 04:04:10,539 - INFO - Loaded 100000 documents...\n",
      "Loading documents: 194265it [00:03, 70662.51it/s]2025-07-20 04:04:12,547 - INFO - Loaded 200000 documents...\n",
      "Loading documents: 293694it [00:05, 69771.62it/s]2025-07-20 04:04:14,303 - INFO - Loaded 300000 documents...\n",
      "Loading documents: 394017it [00:06, 72455.46it/s]2025-07-20 04:04:16,104 - INFO - Loaded 400000 documents...\n",
      "Loading documents: 493173it [00:08, 72132.79it/s]2025-07-20 04:04:17,916 - INFO - Loaded 500000 documents...\n",
      "Loading documents: 598715it [00:10, 74500.08it/s]2025-07-20 04:04:19,811 - INFO - Loaded 600000 documents...\n",
      "Loading documents: 697691it [00:12, 74096.32it/s]2025-07-20 04:04:21,807 - INFO - Loaded 700000 documents...\n",
      "Loading documents: 794912it [00:14, 71953.26it/s]2025-07-20 04:04:23,924 - INFO - Loaded 800000 documents...\n",
      "Loading documents: 897399it [00:16, 70865.27it/s]2025-07-20 04:04:26,135 - INFO - Loaded 900000 documents...\n",
      "Loading documents: 996882it [00:19, 58222.34it/s]2025-07-20 04:04:28,676 - INFO - Loaded 1000000 documents...\n",
      "Loading documents: 1098104it [00:22, 58223.60it/s]2025-07-20 04:04:31,507 - INFO - Loaded 1100000 documents...\n",
      "Loading documents: 1199041it [00:25, 62948.65it/s]2025-07-20 04:04:34,306 - INFO - Loaded 1200000 documents...\n",
      "Loading documents: 1296389it [00:27, 57103.20it/s]2025-07-20 04:04:37,125 - INFO - Loaded 1300000 documents...\n",
      "Loading documents: 1397999it [00:30, 60712.75it/s]2025-07-20 04:04:40,246 - INFO - Loaded 1400000 documents...\n",
      "Loading documents: 1404152it [00:32, 10808.23it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "import ir_datasets\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('msmarco_evaluation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BM25:\n",
    "    \"\"\"Optimized BM25 implementation with proper vectorization\"\"\"\n",
    "\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.corpus_size = 0\n",
    "        self.avg_doc_len = 0\n",
    "        self.doc_lens = []\n",
    "        self.idf = {}\n",
    "        self.doc_term_freqs = []\n",
    "\n",
    "    def fit(self, corpus: List[List[str]]):\n",
    "        \"\"\"Fit BM25 on tokenized corpus\"\"\"\n",
    "        logger.info(f\"Fitting BM25 model on {len(corpus)} documents...\")\n",
    "        self.corpus_size = len(corpus)\n",
    "\n",
    "        # Calculate document lengths\n",
    "        self.doc_lens = [len(doc) for doc in corpus]\n",
    "        self.avg_doc_len = sum(self.doc_lens) / len(self.doc_lens) if self.doc_lens else 0\n",
    "\n",
    "        logger.info(f\"Average document length: {self.avg_doc_len:.2f} tokens\")\n",
    "\n",
    "        # Calculate term frequencies for each document\n",
    "        self.doc_term_freqs = []\n",
    "        df = defaultdict(int)  # document frequency\n",
    "\n",
    "        logger.info(\"Computing term frequencies...\")\n",
    "        for i, doc in enumerate(tqdm(corpus, desc=\"Processing documents for BM25\")):\n",
    "            term_freq = defaultdict(int)\n",
    "            for term in doc:\n",
    "                term_freq[term] += 1\n",
    "            self.doc_term_freqs.append(dict(term_freq))\n",
    "\n",
    "            # Count document frequency\n",
    "            for term in set(doc):\n",
    "                df[term] += 1\n",
    "\n",
    "            # Memory cleanup every 100k documents\n",
    "            if i % 100000 == 0 and i > 0:\n",
    "                logger.info(f\"Processed {i} documents for BM25...\")\n",
    "                gc.collect()\n",
    "\n",
    "        # Calculate IDF\n",
    "        logger.info(f\"Computing IDF scores for {len(df)} unique terms...\")\n",
    "        for term, freq in df.items():\n",
    "            self.idf[term] = log(self.corpus_size / freq)\n",
    "\n",
    "        logger.info(f\"BM25 model fitted successfully with {len(self.idf)} terms\")\n",
    "\n",
    "    def get_scores(self, query: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get BM25 scores for query against all documents\"\"\"\n",
    "        scores = np.zeros(self.corpus_size)\n",
    "\n",
    "        for i in range(self.corpus_size):\n",
    "            score = 0\n",
    "            doc_len = self.doc_lens[i]\n",
    "\n",
    "            for term in query:\n",
    "                if term in self.doc_term_freqs[i] and term in self.idf:\n",
    "                    tf = self.doc_term_freqs[i][term]\n",
    "                    idf = self.idf[term]\n",
    "\n",
    "                    # BM25 formula\n",
    "                    numerator = tf * (self.k1 + 1)\n",
    "                    denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len)\n",
    "                    score += idf * (numerator / denominator)\n",
    "\n",
    "            scores[i] = score\n",
    "\n",
    "        return scores\n",
    "\n",
    "class MSMarcoEvaluator:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", save_dir: str = \"./msmarco_full_eval\"):\n",
    "        \"\"\"\n",
    "        Initialize evaluator for FULL MS MARCO dataset using robust data loading\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model: Optional[SentenceTransformer] = None\n",
    "        self.dense_index: Optional[faiss.Index] = None\n",
    "        self.bm25: Optional[BM25] = None\n",
    "\n",
    "        # Document storage (train set - passages)\n",
    "        self.documents: List[str] = []\n",
    "        self.doc_ids: List[str] = []\n",
    "        self.doc_id_to_index: Dict[str, int] = {}\n",
    "        self.tokenized_docs: List[List[str]] = []\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "        # Query and relevance storage (dev set)\n",
    "        self.dev_queries: Dict[str, str] = {}\n",
    "        self.qrels: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Initializing MS MARCO Evaluator with save directory: {save_dir}\")\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading sentence transformer model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            logger.info(f\"Model loaded successfully. Max sequence length: {self.model.max_seq_length}\")\n",
    "\n",
    "            # Log device information\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            if device == 'cuda':\n",
    "                logger.info(f\"CUDA available - GPU: {torch.cuda.get_device_name()}\")\n",
    "                logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "            else:\n",
    "                logger.info(\"Using CPU for inference\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Enhanced text preprocessing for BM25.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Remove special characters and convert to lowercase\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        # Split and filter tokens\n",
    "        tokens = [token for token in text.split() if len(token) >= 2 and len(token) <= 20]\n",
    "        return tokens\n",
    "\n",
    "    def _safe_text_processing(self, text: str) -> str:\n",
    "        \"\"\"Safely process text to handle encoding issues.\"\"\"\n",
    "        try:\n",
    "            if isinstance(text, bytes):\n",
    "                text = text.decode('utf-8', errors='ignore')\n",
    "            elif not isinstance(text, str):\n",
    "                text = str(text)\n",
    "\n",
    "            # Clean the text\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII\n",
    "            text = ' '.join(text.split())  # Normalize whitespace\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing text: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def load_full_msmarco_data(self):\n",
    "        \"\"\"Load the FULL MS MARCO dataset using HuggingFace datasets for reliability\"\"\"\n",
    "        logger.info(\"Loading FULL MS MARCO dataset...\")\n",
    "\n",
    "        # Check if we have cached data\n",
    "        if self.load_cached_data():\n",
    "            logger.info(\"Successfully loaded cached MS MARCO data\")\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            logger.info(\"=\"*80)\n",
    "            logger.info(\"LOADING MS MARCO PASSAGE RANKING DATASET - FULL VERSION\")\n",
    "            logger.info(\"This will download and process the complete dataset\")\n",
    "            logger.info(\"=\"*80)\n",
    "\n",
    "            # Method 1: Load using HuggingFace datasets (most reliable)\n",
    "            logger.info(\"Loading MS MARCO collection using HuggingFace datasets...\")\n",
    "            \n",
    "            # Load the collection (passages/documents)\n",
    "            logger.info(\"Step 1: Loading passage collection...\")\n",
    "            try:\n",
    "                collection_dataset = load_dataset(\"microsoft/msmarco\", \"collection\", split=\"collection\")\n",
    "                logger.info(f\"Loaded collection with {len(collection_dataset)} passages\")\n",
    "                \n",
    "                # Process collection in batches\n",
    "                batch_size = 50000\n",
    "                processed_count = 0\n",
    "                \n",
    "                for i in tqdm(range(0, len(collection_dataset), batch_size), desc=\"Processing collection\"):\n",
    "                    batch = collection_dataset[i:i+batch_size]\n",
    "                    \n",
    "                    for j, (doc_id, text) in enumerate(zip(batch['id'], batch['text'])):\n",
    "                        clean_text = self._safe_text_processing(text)\n",
    "                        if clean_text and len(clean_text) > 10:\n",
    "                            self.documents.append(clean_text)\n",
    "                            self.doc_ids.append(str(doc_id))\n",
    "                            self.doc_id_to_index[str(doc_id)] = len(self.documents) - 1\n",
    "                            processed_count += 1\n",
    "                    \n",
    "                    # Log progress and save intermediate data\n",
    "                    if processed_count % 100000 == 0:\n",
    "                        logger.info(f\"Processed {processed_count} documents...\")\n",
    "                        self._save_intermediate_data()\n",
    "                        gc.collect()\n",
    "                \n",
    "                logger.info(f\"Collection loaded: {len(self.documents)} documents\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading collection via HuggingFace: {e}\")\n",
    "                # Fallback to ir_datasets\n",
    "                logger.info(\"Falling back to ir_datasets...\")\n",
    "                return self._load_with_ir_datasets()\n",
    "\n",
    "            # Step 2: Load queries\n",
    "            logger.info(\"Step 2: Loading dev queries...\")\n",
    "            try:\n",
    "                queries_dataset = load_dataset(\"microsoft/msmarco\", \"queries.dev\", split=\"dev\")\n",
    "                logger.info(f\"Loaded {len(queries_dataset)} dev queries\")\n",
    "                \n",
    "                for item in tqdm(queries_dataset, desc=\"Processing queries\"):\n",
    "                    query_id = str(item['id'])\n",
    "                    query_text = self._safe_text_processing(item['text'])\n",
    "                    if query_text:\n",
    "                        self.dev_queries[query_id] = query_text\n",
    "                        \n",
    "                logger.info(f\"Dev queries loaded: {len(self.dev_queries)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading queries: {e}\")\n",
    "                # Try alternative approach\n",
    "                try:\n",
    "                    queries_dataset = load_dataset(\"microsoft/msmarco-dev\", \"queries\", split=\"dev\")\n",
    "                    for item in tqdm(queries_dataset, desc=\"Processing queries (alt method)\"):\n",
    "                        query_id = str(item['query_id'])\n",
    "                        query_text = self._safe_text_processing(item['query'])\n",
    "                        if query_text:\n",
    "                            self.dev_queries[query_id] = query_text\n",
    "                    logger.info(f\"Dev queries loaded (alt): {len(self.dev_queries)}\")\n",
    "                except Exception as e2:\n",
    "                    logger.warning(f\"Could not load queries: {e2}\")\n",
    "\n",
    "            # Step 3: Load qrels\n",
    "            logger.info(\"Step 3: Loading relevance judgments (qrels)...\")\n",
    "            try:\n",
    "                qrels_dataset = load_dataset(\"microsoft/msmarco\", \"qrels.dev\", split=\"dev\")\n",
    "                logger.info(f\"Loaded {len(qrels_dataset)} qrels\")\n",
    "                \n",
    "                for item in tqdm(qrels_dataset, desc=\"Processing qrels\"):\n",
    "                    query_id = str(item['query_id'])\n",
    "                    doc_id = str(item['corpus_id'])\n",
    "                    relevance = int(item['score'])\n",
    "                    \n",
    "                    if query_id not in self.qrels:\n",
    "                        self.qrels[query_id] = {}\n",
    "                    self.qrels[query_id][doc_id] = relevance\n",
    "                    \n",
    "                logger.info(f\"Qrels loaded: {len(self.qrels)} queries with relevance judgments\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading qrels: {e}\")\n",
    "                # Try alternative\n",
    "                try:\n",
    "                    qrels_dataset = load_dataset(\"microsoft/msmarco-dev\", \"qrels\", split=\"dev\")\n",
    "                    for item in tqdm(qrels_dataset, desc=\"Processing qrels (alt)\"):\n",
    "                        query_id = str(item['query_id'])\n",
    "                        doc_id = str(item['corpus_id'])\n",
    "                        relevance = int(item['score'])\n",
    "                        \n",
    "                        if query_id not in self.qrels:\n",
    "                            self.qrels[query_id] = {}\n",
    "                        self.qrels[query_id][doc_id] = relevance\n",
    "                    logger.info(f\"Qrels loaded (alt): {len(self.qrels)}\")\n",
    "                except Exception as e2:\n",
    "                    logger.warning(f\"Could not load qrels: {e2}\")\n",
    "\n",
    "            # Step 4: Tokenize documents for BM25\n",
    "            logger.info(\"Step 4: Tokenizing documents for BM25...\")\n",
    "            self.tokenized_docs = []\n",
    "            batch_size = 10000\n",
    "\n",
    "            for i in tqdm(range(0, len(self.documents), batch_size), desc=\"Tokenizing documents\"):\n",
    "                batch = self.documents[i:i+batch_size]\n",
    "                tokenized_batch = [self._preprocess_text(doc) for doc in batch]\n",
    "                self.tokenized_docs.extend(tokenized_batch)\n",
    "\n",
    "                # Memory cleanup\n",
    "                if i % 100000 == 0 and i > 0:\n",
    "                    logger.info(f\"Tokenized {i} documents...\")\n",
    "                    gc.collect()\n",
    "\n",
    "            # Save all data\n",
    "            self._save_all_data()\n",
    "\n",
    "            logger.info(\"=\"*80)\n",
    "            logger.info(\"MS MARCO DATASET LOADING COMPLETED!\")\n",
    "            logger.info(f\"Documents: {len(self.documents):,}\")\n",
    "            logger.info(f\"Queries: {len(self.dev_queries):,}\")\n",
    "            logger.info(f\"QRels: {len(self.qrels):,}\")\n",
    "            logger.info(\"=\"*80)\n",
    "            \n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading MS MARCO dataset: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _load_with_ir_datasets(self):\n",
    "        \"\"\"Fallback method using ir_datasets with proper error handling\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading with ir_datasets as fallback...\")\n",
    "            \n",
    "            # Try different dataset variations\n",
    "            dataset_names = [\n",
    "                \"msmarco-passage\",\n",
    "                \"msmarco-passage/dev\",\n",
    "                \"msmarco-passage/train\"\n",
    "            ]\n",
    "            \n",
    "            dataset = None\n",
    "            for dataset_name in dataset_names:\n",
    "                try:\n",
    "                    logger.info(f\"Trying to load dataset: {dataset_name}\")\n",
    "                    dataset = ir_datasets.load(dataset_name)\n",
    "                    logger.info(f\"Successfully loaded: {dataset_name}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {dataset_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if dataset is None:\n",
    "                logger.error(\"Could not load any MS MARCO dataset variant\")\n",
    "                return False\n",
    "            \n",
    "            # Load documents\n",
    "            logger.info(\"Loading documents...\")\n",
    "            doc_count = 0\n",
    "            try:\n",
    "                if hasattr(dataset, 'docs_iter'):\n",
    "                    for doc in tqdm(dataset.docs_iter(), desc=\"Loading documents\"):\n",
    "                        try:\n",
    "                            clean_text = self._safe_text_processing(doc.text)\n",
    "                            if clean_text and len(clean_text) > 10:\n",
    "                                self.documents.append(clean_text)\n",
    "                                self.doc_ids.append(doc.doc_id)\n",
    "                                self.doc_id_to_index[doc.doc_id] = len(self.documents) - 1\n",
    "                                doc_count += 1\n",
    "                                \n",
    "                                if doc_count % 100000 == 0:\n",
    "                                    logger.info(f\"Loaded {doc_count} documents...\")\n",
    "                                    self._save_intermediate_data()\n",
    "                                    gc.collect()\n",
    "                                    \n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error processing document: {e}\")\n",
    "                            continue\n",
    "                else:\n",
    "                    logger.warning(\"Dataset does not have docs_iter method\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading documents: {e}\")\n",
    "            \n",
    "            logger.info(f\"Loaded {len(self.documents)} documents\")\n",
    "            \n",
    "            # Load queries with multiple approaches\n",
    "            logger.info(\"Loading queries...\")\n",
    "            queries_loaded = False\n",
    "            \n",
    "            # Method 1: Try queries_iter\n",
    "            try:\n",
    "                if hasattr(dataset, 'queries_iter'):\n",
    "                    for query in tqdm(dataset.queries_iter(), desc=\"Loading queries\"):\n",
    "                        try:\n",
    "                            clean_query = self._safe_text_processing(query.text)\n",
    "                            if clean_query:\n",
    "                                self.dev_queries[query.query_id] = clean_query\n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error processing query: {e}\")\n",
    "                            continue\n",
    "                    queries_loaded = True\n",
    "                    logger.info(f\"Loaded {len(self.dev_queries)} queries via queries_iter\")\n",
    "                else:\n",
    "                    logger.warning(\"Dataset does not have queries_iter method\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"queries_iter failed: {e}\")\n",
    "            \n",
    "            # Method 2: Try loading dev queries separately\n",
    "            if not queries_loaded:\n",
    "                try:\n",
    "                    logger.info(\"Trying to load dev queries separately...\")\n",
    "                    dev_dataset = ir_datasets.load(\"msmarco-passage/dev\")\n",
    "                    if hasattr(dev_dataset, 'queries_iter'):\n",
    "                        for query in tqdm(dev_dataset.queries_iter(), desc=\"Loading dev queries\"):\n",
    "                            try:\n",
    "                                clean_query = self._safe_text_processing(query.text)\n",
    "                                if clean_query:\n",
    "                                    self.dev_queries[query.query_id] = clean_query\n",
    "                            except Exception as e:\n",
    "                                continue\n",
    "                        queries_loaded = True\n",
    "                        logger.info(f\"Loaded {len(self.dev_queries)} dev queries\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Dev queries loading failed: {e}\")\n",
    "            \n",
    "            # Method 3: Try accessing queries as a dataset property\n",
    "            if not queries_loaded:\n",
    "                try:\n",
    "                    logger.info(\"Trying to access queries as dataset property...\")\n",
    "                    if hasattr(dataset, 'queries'):\n",
    "                        queries_data = dataset.queries\n",
    "                        for query_id, query_text in queries_data.items():\n",
    "                            clean_query = self._safe_text_processing(query_text)\n",
    "                            if clean_query:\n",
    "                                self.dev_queries[query_id] = clean_query\n",
    "                        queries_loaded = True\n",
    "                        logger.info(f\"Loaded {len(self.dev_queries)} queries via dataset property\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Dataset property access failed: {e}\")\n",
    "            \n",
    "            if not queries_loaded:\n",
    "                logger.warning(\"Could not load queries via ir_datasets. Will continue with documents only.\")\n",
    "            \n",
    "            # Load qrels with multiple approaches\n",
    "            logger.info(\"Loading qrels...\")\n",
    "            qrels_loaded = False\n",
    "            \n",
    "            # Method 1: Try qrels_iter\n",
    "            try:\n",
    "                if hasattr(dataset, 'qrels_iter'):\n",
    "                    for qrel in tqdm(dataset.qrels_iter(), desc=\"Loading qrels\"):\n",
    "                        try:\n",
    "                            if qrel.query_id not in self.qrels:\n",
    "                                self.qrels[qrel.query_id] = {}\n",
    "                            self.qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error processing qrel: {e}\")\n",
    "                            continue\n",
    "                    qrels_loaded = True\n",
    "                    logger.info(f\"Loaded qrels for {len(self.qrels)} queries\")\n",
    "                else:\n",
    "                    logger.warning(\"Dataset does not have qrels_iter method\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"qrels_iter failed: {e}\")\n",
    "            \n",
    "            # Method 2: Try loading dev qrels separately\n",
    "            if not qrels_loaded:\n",
    "                try:\n",
    "                    logger.info(\"Trying to load dev qrels separately...\")\n",
    "                    dev_dataset = ir_datasets.load(\"msmarco-passage/dev\")\n",
    "                    if hasattr(dev_dataset, 'qrels_iter'):\n",
    "                        for qrel in tqdm(dev_dataset.qrels_iter(), desc=\"Loading dev qrels\"):\n",
    "                            try:\n",
    "                                if qrel.query_id not in self.qrels:\n",
    "                                    self.qrels[qrel.query_id] = {}\n",
    "                                self.qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
    "                            except Exception as e:\n",
    "                                continue\n",
    "                        qrels_loaded = True\n",
    "                        logger.info(f\"Loaded dev qrels for {len(self.qrels)} queries\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Dev qrels loading failed: {e}\")\n",
    "            \n",
    "            if not qrels_loaded:\n",
    "                logger.warning(\"Could not load qrels via ir_datasets.\")\n",
    "            \n",
    "            # If we have documents but no queries/qrels, try to load them via HuggingFace as a final attempt\n",
    "            if len(self.documents) > 0 and (len(self.dev_queries) == 0 or len(self.qrels) == 0):\n",
    "                logger.info(\"Attempting to load queries/qrels via HuggingFace as final fallback...\")\n",
    "                try:\n",
    "                    # Load queries\n",
    "                    if len(self.dev_queries) == 0:\n",
    "                        queries_dataset = load_dataset(\"microsoft/msmarco\", \"queries.dev\", split=\"dev\")\n",
    "                        for item in tqdm(queries_dataset, desc=\"Loading HF queries\"):\n",
    "                            query_id = str(item['id'])\n",
    "                            query_text = self._safe_text_processing(item['text'])\n",
    "                            if query_text:\n",
    "                                self.dev_queries[query_id] = query_text\n",
    "                        logger.info(f\"Loaded {len(self.dev_queries)} queries via HuggingFace\")\n",
    "                    \n",
    "                    # Load qrels\n",
    "                    if len(self.qrels) == 0:\n",
    "                        qrels_dataset = load_dataset(\"microsoft/msmarco\", \"qrels.dev\", split=\"dev\")\n",
    "                        for item in tqdm(qrels_dataset, desc=\"Loading HF qrels\"):\n",
    "                            query_id = str(item['query_id'])\n",
    "                            doc_id = str(item['corpus_id'])\n",
    "                            relevance = int(item['score'])\n",
    "                            \n",
    "                            if query_id not in self.qrels:\n",
    "                                self.qrels[query_id] = {}\n",
    "                            self.qrels[query_id][doc_id] = relevance\n",
    "                        logger.info(f\"Loaded qrels for {len(self.qrels)} queries via HuggingFace\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"HuggingFace fallback also failed: {e}\")\n",
    "            \n",
    "            success = len(self.documents) > 0\n",
    "            if success:\n",
    "                logger.info(f\"ir_datasets loading completed with:\")\n",
    "                logger.info(f\"- Documents: {len(self.documents)}\")\n",
    "                logger.info(f\"- Queries: {len(self.dev_queries)}\")\n",
    "                logger.info(f\"- QRels: {len(self.qrels)}\")\n",
    "            \n",
    "            return success\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ir_datasets fallback failed completely: {e}\")\n",
    "            return False\n",
    "    def _save_intermediate_data(self):\n",
    "        \"\"\"Save intermediate data during loading\"\"\"\n",
    "        try:\n",
    "            temp_path = os.path.join(self.save_dir, 'temp_documents.pkl')\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'documents': self.documents,\n",
    "                    'doc_ids': self.doc_ids,\n",
    "                    'doc_id_to_index': self.doc_id_to_index\n",
    "                }, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error saving intermediate data: {e}\")\n",
    "\n",
    "    def _save_all_data(self):\n",
    "        \"\"\"Save all loaded data to cache\"\"\"\n",
    "        try:\n",
    "            # Save documents\n",
    "            docs_path = os.path.join(self.save_dir, 'full_documents.pkl')\n",
    "            logger.info(f\"Saving {len(self.documents)} documents to cache...\")\n",
    "            with open(docs_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'documents': self.documents,\n",
    "                    'doc_ids': self.doc_ids,\n",
    "                    'doc_id_to_index': self.doc_id_to_index,\n",
    "                    'tokenized_docs': self.tokenized_docs\n",
    "                }, f)\n",
    "\n",
    "            # Save queries and qrels\n",
    "            queries_path = os.path.join(self.save_dir, 'dev_queries.pkl')\n",
    "            with open(queries_path, 'wb') as f:\n",
    "                pickle.dump(self.dev_queries, f)\n",
    "\n",
    "            qrels_path = os.path.join(self.save_dir, 'qrels.pkl')\n",
    "            with open(qrels_path, 'wb') as f:\n",
    "                pickle.dump(self.qrels, f)\n",
    "\n",
    "            logger.info(\"All data saved to cache successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data to cache: {e}\")\n",
    "\n",
    "    def load_cached_data(self) -> bool:\n",
    "        \"\"\"Load cached data if available\"\"\"\n",
    "        docs_path = os.path.join(self.save_dir, 'full_documents.pkl')\n",
    "        queries_path = os.path.join(self.save_dir, 'dev_queries.pkl')\n",
    "        qrels_path = os.path.join(self.save_dir, 'qrels.pkl')\n",
    "\n",
    "        if all(os.path.exists(p) for p in [docs_path, queries_path, qrels_path]):\n",
    "            try:\n",
    "                logger.info(\"Loading cached data...\")\n",
    "\n",
    "                # Load documents\n",
    "                with open(docs_path, 'rb') as f:\n",
    "                    doc_data = pickle.load(f)\n",
    "                    self.documents = doc_data['documents']\n",
    "                    self.doc_ids = doc_data['doc_ids']\n",
    "                    self.doc_id_to_index = doc_data['doc_id_to_index']\n",
    "                    self.tokenized_docs = doc_data.get('tokenized_docs', [])\n",
    "\n",
    "                # Load queries\n",
    "                with open(queries_path, 'rb') as f:\n",
    "                    self.dev_queries = pickle.load(f)\n",
    "\n",
    "                # Load qrels\n",
    "                with open(qrels_path, 'rb') as f:\n",
    "                    self.qrels = pickle.load(f)\n",
    "\n",
    "                logger.info(f\"Cached data loaded: {len(self.documents)} docs, {len(self.dev_queries)} queries, {len(self.qrels)} qrel sets\")\n",
    "\n",
    "                # Tokenize documents if not cached\n",
    "                if not self.tokenized_docs:\n",
    "                    logger.info(\"Tokenizing documents for BM25...\")\n",
    "                    self.tokenized_docs = []\n",
    "                    batch_size = 10000\n",
    "                    for i in tqdm(range(0, len(self.documents), batch_size), desc=\"Tokenizing\"):\n",
    "                        batch = self.documents[i:i+batch_size]\n",
    "                        tokenized_batch = [self._preprocess_text(doc) for doc in batch]\n",
    "                        self.tokenized_docs.extend(tokenized_batch)\n",
    "                        \n",
    "                        if i % 100000 == 0 and i > 0:\n",
    "                            gc.collect()\n",
    "                    \n",
    "                    # Save updated cache\n",
    "                    with open(docs_path, 'wb') as f:\n",
    "                        pickle.dump({\n",
    "                            'documents': self.documents,\n",
    "                            'doc_ids': self.doc_ids,\n",
    "                            'doc_id_to_index': self.doc_id_to_index,\n",
    "                            'tokenized_docs': self.tokenized_docs\n",
    "                        }, f)\n",
    "\n",
    "                return True\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cached data: {e}\")\n",
    "                return False\n",
    "\n",
    "        return False\n",
    "\n",
    "    def build_dense_index(self, batch_size: int = 64):\n",
    "        \"\"\"Build optimized dense FAISS index with IVF for large collection\"\"\"\n",
    "        logger.info(\"Building dense FAISS index...\")\n",
    "\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents loaded\")\n",
    "\n",
    "        embeddings_path = os.path.join(self.save_dir, 'embeddings.npy')\n",
    "        index_path = os.path.join(self.save_dir, 'faiss_index.bin')\n",
    "\n",
    "        # Try to load existing embeddings and index\n",
    "        if os.path.exists(embeddings_path) and os.path.exists(index_path):\n",
    "            try:\n",
    "                logger.info(\"Loading existing embeddings and index...\")\n",
    "                self.embeddings = np.load(embeddings_path)\n",
    "                self.dense_index = faiss.read_index(index_path)\n",
    "\n",
    "                if len(self.embeddings) == len(self.documents):\n",
    "                    logger.info(f\"Loaded existing index with {self.dense_index.ntotal} documents\")\n",
    "                    return\n",
    "                else:\n",
    "                    logger.warning(\"Embedding count mismatch. Rebuilding...\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cached index: {e}. Rebuilding...\")\n",
    "\n",
    "        # Generate embeddings with memory management\n",
    "        logger.info(\"Generating embeddings...\")\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        # Process in chunks to handle memory\n",
    "        chunk_size = 25000 if device == 'cuda' else 5000\n",
    "        all_embeddings = []\n",
    "\n",
    "        logger.info(f\"Processing {len(self.documents)} documents in chunks of {chunk_size}\")\n",
    "\n",
    "        for i in tqdm(range(0, len(self.documents), chunk_size), desc=\"Generating embeddings\"):\n",
    "            chunk_docs = self.documents[i:i+chunk_size]\n",
    "            try:\n",
    "                chunk_embeddings = self.model.encode(\n",
    "                    chunk_docs,\n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False,\n",
    "                    batch_size=batch_size,\n",
    "                    device=device,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "                all_embeddings.append(chunk_embeddings)\n",
    "                \n",
    "                # Log progress\n",
    "                if i % 100000 == 0:\n",
    "                    logger.info(f\"Generated embeddings for {i + len(chunk_docs)} documents\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating embeddings for chunk {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        logger.info(\"Concatenating embeddings...\")\n",
    "        self.embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "        # Save embeddings\n",
    "        logger.info(\"Saving embeddings...\")\n",
    "        np.save(embeddings_path, self.embeddings)\n",
    "\n",
    "        # Build IVF index for large collection\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        num_docs = len(self.documents)\n",
    "\n",
    "        logger.info(f\"Building IVF index for {num_docs} documents with dimension {dimension}\")\n",
    "\n",
    "        # Calculate optimal number of centroids\n",
    "        n_list = min(max(int(4 * np.sqrt(num_docs)), 4000), num_docs // 50)\n",
    "        logger.info(f\"Using {n_list} centroids for IVF index\")\n",
    "\n",
    "        # Create IVF index\n",
    "        quantizer = faiss.IndexFlatIP(dimension)\n",
    "        self.dense_index = faiss.IndexIVFFlat(quantizer, dimension, n_list, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "        # Train the index\n",
    "        logger.info(f\"Training IVF index...\")\n",
    "        training_data = self.embeddings[::max(1, len(self.embeddings) // 1000000)]  # Sample for training\n",
    "        self.dense_index.train(training_data)\n",
    "\n",
    "        # Add vectors to index in batches\n",
    "        logger.info(\"Adding vectors to index...\")\n",
    "        batch_size_add = 100000\n",
    "        for i in tqdm(range(0, len(self.embeddings), batch_size_add), desc=\"Adding to index\"):\n",
    "            end_idx = min(i + batch_size_add, len(self.embeddings))\n",
    "            self.dense_index.add(self.embeddings[i:end_idx])\n",
    "            \n",
    "            if i % 500000 == 0 and i > 0:\n",
    "                logger.info(f\"Added {i} vectors to index\")\n",
    "\n",
    "        # Save index\n",
    "        faiss.write_index(self.dense_index, index_path)\n",
    "        logger.info(f\"IVF index built and saved with {self.dense_index.ntotal} documents\")\n",
    "\n",
    "    def build_bm25_index(self):\n",
    "        \"\"\"Build BM25 index\"\"\"\n",
    "        logger.info(\"Building BM25 index...\")\n",
    "\n",
    "        bm25_path = os.path.join(self.save_dir, 'bm25_model.pkl')\n",
    "\n",
    "        # Try to load existing BM25 model\n",
    "        if os.path.exists(bm25_path):\n",
    "            try:\n",
    "                logger.info(\"Loading existing BM25 model...\")\n",
    "                with open(bm25_path, 'rb') as f:\n",
    "                    self.bm25 = pickle.load(f)\n",
    "                logger.info(\"Loaded existing BM25 model\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading cached BM25 model: {e}. Rebuilding...\")\n",
    "\n",
    "        if not self.tokenized_docs:\n",
    "            raise ValueError(\"Documents must be loaded and tokenized before building BM25 index.\")\n",
    "\n",
    "        # Build BM25 model\n",
    "        self.bm25 = BM25()\n",
    "        self.bm25.fit(self.tokenized_docs)\n",
    "\n",
    "        # Save BM25 model\n",
    "        try:\n",
    "            logger.info(\"Saving BM25 model...\")\n",
    "            with open(bm25_path, 'wb') as f:\n",
    "                pickle.dump(self.bm25, f)\n",
    "            logger.info(\"BM25 index built and cached successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving BM25 model: {e}\")\n",
    "\n",
    "    def search_dense(self, query: str, k: int = 1000) -> List[Tuple[str, float, str]]:\n",
    "        \"\"\"Dense retrieval using semantic similarity with IVF search\"\"\"\n",
    "        if self.dense_index is None:\n",
    "            raise ValueError(\"Dense index not built\")\n",
    "\n",
    "        # Set nprobe for IVF index (search more centroids for better recall)\n",
    "        if hasattr(self.dense_index, 'nprobe'):\n",
    "            self.dense_index.nprobe = min(100, self.dense_index.nlist // 2)\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.model.encode(\n",
    "            [query],\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # Search\n",
    "        scores, indices = self.dense_index.search(query_embedding, k)\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if 0 <= idx < len(self.doc_ids):\n",
    "                results.append((\n",
    "                    self.doc_ids[idx],\n",
    "                    float(score),\n",
    "                    self.documents[idx]\n",
    "                ))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_bm25(self, query: str, k: int = 1000) -> List[Tuple[str, float, str]]:\n",
    "        \"\"\"Sparse retrieval using BM25\"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25 index not built. Call build_bm25_index() first.\")\n",
    "\n",
    "        tokenized_query = self._preprocess_text(query)\n",
    "        if not tokenized_query:\n",
    "            return []\n",
    "\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0:\n",
    "                results.append((\n",
    "                    self.doc_ids[idx],\n",
    "                    float(scores[idx]),\n",
    "                    self.documents[idx]\n",
    "                ))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_hybrid(self, query: str, k: int = 100, alpha: float = 0.5, candidate_k: int = 500) -> List[Tuple[str, float, str]]:\n",
    "        \"\"\"\n",
    "        Hybrid retrieval: Get 500 candidates from dense, then re-rank with BM25\n",
    "        \"\"\"\n",
    "        if self.dense_index is None or self.bm25 is None:\n",
    "            raise ValueError(\"Both dense and BM25 indices must be built for hybrid search.\")\n",
    "\n",
    "        # Step 1: Get candidates using dense retrieval\n",
    "        logger.debug(f\"Getting {candidate_k} candidates from dense search...\")\n",
    "        dense_candidates = self.search_dense(query, k=candidate_k)\n",
    "\n",
    "        if not dense_candidates:\n",
    "            return []\n",
    "\n",
    "        # Step 2: Create BM25 scoring on candidates\n",
    "        candidate_doc_ids = [doc_id for doc_id, _, _ in dense_candidates]\n",
    "        candidate_indices = [self.doc_id_to_index[doc_id] for doc_id in candidate_doc_ids]\n",
    "\n",
    "        # Get BM25 scores for query\n",
    "        tokenized_query = self._preprocess_text(query)\n",
    "        if not tokenized_query:\n",
    "            return dense_candidates[:k]\n",
    "\n",
    "        # Compute BM25 scores for candidates only\n",
    "        candidate_bm25_scores = {}\n",
    "        for idx in candidate_indices:\n",
    "            score = 0\n",
    "            doc_len = self.bm25.doc_lens[idx]\n",
    "\n",
    "            for term in tokenized_query:\n",
    "                if term in self.bm25.doc_term_freqs[idx] and term in self.bm25.idf:\n",
    "                    tf = self.bm25.doc_term_freqs[idx][term]\n",
    "                    idf = self.bm25.idf[term]\n",
    "\n",
    "                    numerator = tf * (self.bm25.k1 + 1)\n",
    "                    denominator = tf + self.bm25.k1 * (1 - self.bm25.b + self.bm25.b * doc_len / self.bm25.avg_doc_len)\n",
    "                    score += idf * (numerator / denominator)\n",
    "\n",
    "            candidate_bm25_scores[self.doc_ids[idx]] = score\n",
    "\n",
    "        # Step 3: Combine scores\n",
    "        dense_scores = [score for _, score, _ in dense_candidates]\n",
    "        bm25_scores = [candidate_bm25_scores.get(doc_id, 0.0) for doc_id, _, _ in dense_candidates]\n",
    "\n",
    "        # Min-max normalization\n",
    "        def normalize_scores(scores):\n",
    "            if not scores or max(scores) == min(scores):\n",
    "                return [0.0] * len(scores)\n",
    "            min_s, max_s = min(scores), max(scores)\n",
    "            return [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "\n",
    "        norm_dense = normalize_scores(dense_scores)\n",
    "        norm_bm25 = normalize_scores(bm25_scores)\n",
    "\n",
    "        # Combine scores\n",
    "        combined_results = []\n",
    "        for i, (doc_id, _, text) in enumerate(dense_candidates):\n",
    "           combined_score = alpha * norm_dense[i] + (1 - alpha) * norm_bm25[i]\n",
    "           combined_results.append((doc_id, combined_score, text))\n",
    "\n",
    "       # Sort and return top k\n",
    "        combined_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return combined_results[:k]\n",
    "\n",
    "    def evaluate_on_dev_set(self, max_queries: int = None, sample_queries: bool = True):\n",
    "       \"\"\"Evaluate on MS MARCO dev set with comprehensive metrics\"\"\"\n",
    "       logger.info(\"Starting evaluation on MS MARCO dev set...\")\n",
    "\n",
    "       if not self.dev_queries or not self.qrels:\n",
    "           logger.error(\"No dev queries or qrels loaded\")\n",
    "           return {}\n",
    "\n",
    "       # Filter queries that have relevance judgments\n",
    "       valid_queries = {\n",
    "           qid: qtext for qid, qtext in self.dev_queries.items()\n",
    "           if qid in self.qrels and any(rel > 0 for rel in self.qrels[qid].values())\n",
    "       }\n",
    "\n",
    "       logger.info(f\"Found {len(valid_queries)} valid queries with positive relevance judgments\")\n",
    "\n",
    "       # Sample queries for evaluation if needed\n",
    "       if max_queries and len(valid_queries) > max_queries:\n",
    "           if sample_queries:\n",
    "               import random\n",
    "               random.seed(42)  # For reproducibility\n",
    "               sample_queries_dict = dict(random.sample(list(valid_queries.items()), max_queries))\n",
    "               logger.info(f\"Randomly sampling {max_queries} queries for evaluation\")\n",
    "           else:\n",
    "               sample_queries_dict = dict(list(valid_queries.items())[:max_queries])\n",
    "               logger.info(f\"Using first {max_queries} queries for evaluation\")\n",
    "       else:\n",
    "           sample_queries_dict = valid_queries\n",
    "\n",
    "       # Evaluate each method\n",
    "       methods = ['dense', 'bm25', 'hybrid']\n",
    "       cutoffs = [1, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "       results = {}\n",
    "       for method in methods:\n",
    "           logger.info(f\"Evaluating {method} retrieval on {len(sample_queries_dict)} queries...\")\n",
    "           start_time = time.time()\n",
    "\n",
    "           method_results = {\n",
    "               'precision': {k: [] for k in cutoffs},\n",
    "               'recall': {k: [] for k in cutoffs},\n",
    "               'mrr': {k: [] for k in cutoffs},\n",
    "               'ndcg': {k: [] for k in cutoffs},\n",
    "               'map': [],\n",
    "               'total_queries': 0,\n",
    "               'time': 0.0,\n",
    "               'queries_per_second': 0.0,\n",
    "               'avg_query_time': 0.0\n",
    "           }\n",
    "\n",
    "           query_times = []\n",
    "           processed_queries = 0\n",
    "\n",
    "           for query_id, query_text in tqdm(sample_queries_dict.items(), desc=f\"Evaluating {method}\"):\n",
    "               try:\n",
    "                   query_start = time.time()\n",
    "\n",
    "                   # Get search results\n",
    "                   if method == 'dense':\n",
    "                       search_results = self.search_dense(query_text, k=100)\n",
    "                   elif method == 'bm25':\n",
    "                       search_results = self.search_bm25(query_text, k=100)\n",
    "                   elif method == 'hybrid':\n",
    "                       search_results = self.search_hybrid(query_text, k=100, candidate_k=500, alpha=0.5)\n",
    "                   else:\n",
    "                       continue\n",
    "\n",
    "                   query_times.append(time.time() - query_start)\n",
    "\n",
    "                   if not search_results:\n",
    "                       continue\n",
    "\n",
    "                   retrieved_docs = [doc_id for doc_id, _, _ in search_results]\n",
    "                   relevant_docs = {doc_id: rel for doc_id, rel in self.qrels[query_id].items() if rel > 0}\n",
    "\n",
    "                   if not relevant_docs:\n",
    "                       continue\n",
    "\n",
    "                   method_results['total_queries'] += 1\n",
    "                   processed_queries += 1\n",
    "\n",
    "                   # Calculate metrics for each cutoff\n",
    "                   for k in cutoffs:\n",
    "                       retrieved_at_k = retrieved_docs[:k]\n",
    "                       relevant_retrieved = sum(1 for doc_id in retrieved_at_k if doc_id in relevant_docs)\n",
    "\n",
    "                       # Precision@k\n",
    "                       precision = relevant_retrieved / k if k > 0 else 0.0\n",
    "                       method_results['precision'][k].append(precision)\n",
    "\n",
    "                       # Recall@k\n",
    "                       recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0.0\n",
    "                       method_results['recall'][k].append(recall)\n",
    "\n",
    "                       # MRR@k\n",
    "                       mrr = 0.0\n",
    "                       for i, doc_id in enumerate(retrieved_at_k):\n",
    "                           if doc_id in relevant_docs:\n",
    "                               mrr = 1.0 / (i + 1)\n",
    "                               break\n",
    "                       method_results['mrr'][k].append(mrr)\n",
    "\n",
    "                       # NDCG@k\n",
    "                       dcg = sum(\n",
    "                           (2**min(relevant_docs.get(doc_id, 0), 3) - 1) / np.log2(i + 2)\n",
    "                           for i, doc_id in enumerate(retrieved_at_k)\n",
    "                           if doc_id in relevant_docs\n",
    "                       )\n",
    "                       ideal_relevances = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "                       idcg = sum((2**min(rel, 3) - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevances))\n",
    "                       ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "                       method_results['ndcg'][k].append(ndcg)\n",
    "\n",
    "                   # MAP calculation\n",
    "                   if relevant_docs:\n",
    "                       ap = 0.0\n",
    "                       rel_found = 0\n",
    "                       for i, doc_id in enumerate(retrieved_docs):\n",
    "                           if doc_id in relevant_docs:\n",
    "                               rel_found += 1\n",
    "                               ap += rel_found / (i + 1)\n",
    "                       map_score = ap / len(relevant_docs)\n",
    "                       method_results['map'].append(map_score)\n",
    "\n",
    "                   # Log progress every 100 queries\n",
    "                   if processed_queries % 100 == 0:\n",
    "                       logger.info(f\"Processed {processed_queries} queries for {method}...\")\n",
    "\n",
    "               except Exception as e:\n",
    "                   logger.warning(f\"Error processing query {query_id}: {e}\")\n",
    "                   continue\n",
    "\n",
    "           # Calculate final averages\n",
    "           for metric_name in ['precision', 'recall', 'mrr', 'ndcg']:\n",
    "               for k in cutoffs:\n",
    "                   values = method_results[metric_name][k]\n",
    "                   method_results[metric_name][k] = np.mean(values) if values else 0.0\n",
    "\n",
    "           map_values = method_results['map']\n",
    "           method_results['map'] = np.mean(map_values) if map_values else 0.0\n",
    "           method_results['time'] = time.time() - start_time\n",
    "\n",
    "           if query_times:\n",
    "               method_results['queries_per_second'] = len(query_times) / sum(query_times)\n",
    "               method_results['avg_query_time'] = np.mean(query_times)\n",
    "\n",
    "           results[method] = method_results\n",
    "\n",
    "           logger.info(f\"{method} evaluation completed: {method_results['total_queries']} queries, \"\n",
    "                       f\"{method_results['queries_per_second']:.2f} queries/sec, \"\n",
    "                       f\"MAP: {method_results['map']:.4f}\")\n",
    "\n",
    "       return results\n",
    "\n",
    "    def print_evaluation_results(self, results: Dict[str, Any]):\n",
    "       \"\"\"Print comprehensive evaluation results\"\"\"\n",
    "       print(\"\\n\" + \"=\"*120)\n",
    "       print(\"MS MARCO PASSAGE RETRIEVAL EVALUATION RESULTS\")\n",
    "       print(\"=\"*120)\n",
    "\n",
    "       methods = ['dense', 'bm25', 'hybrid']\n",
    "       cutoffs = [1, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "       for method in methods:\n",
    "           if method not in results:\n",
    "               continue\n",
    "\n",
    "           metrics = results[method]\n",
    "           print(f\"\\n{method.upper()} RETRIEVAL RESULTS:\")\n",
    "           print(\"-\" * 60)\n",
    "           print(f\"Total Queries Evaluated: {metrics.get('total_queries', 0)}\")\n",
    "           print(f\"Mean Average Precision (MAP): {metrics['map']:.4f}\")\n",
    "           print(f\"Evaluation Time: {metrics.get('time', 0):.1f} seconds\")\n",
    "           print(f\"Queries per Second: {metrics.get('queries_per_second', 0):.2f}\")\n",
    "           print(f\"Average Query Time: {metrics.get('avg_query_time', 0):.4f} seconds\\n\")\n",
    "\n",
    "           print(f\"{'Metric':<12} {'@1':<8} {'@3':<8} {'@5':<8} {'@10':<8} {'@20':<8} {'@50':<8} {'@100':<8}\")\n",
    "           print(\"-\" * 88)\n",
    "\n",
    "           for metric_name in ['precision', 'recall', 'mrr', 'ndcg']:\n",
    "               values = [f\"{metrics[metric_name][k]:.4f}\" for k in cutoffs]\n",
    "               print(f\"{metric_name.upper():<12} {' '.join(f'{v:<8}' for v in values)}\")\n",
    "\n",
    "       # Summary comparison\n",
    "       print(\"\\n\" + \"=\"*120)\n",
    "       print(\"METHOD COMPARISON SUMMARY\")\n",
    "       print(\"=\"*120)\n",
    "       print(f\"{'Method':<15} {'MAP':<10} {'MRR@10':<10} {'Recall@10':<12} {'NDCG@10':<10} {'QPS':<10} {'Time(s)':<10}\")\n",
    "       print(\"-\" * 80)\n",
    "\n",
    "       for method in methods:\n",
    "           if method in results:\n",
    "               metrics = results[method]\n",
    "               print(f\"{method.upper():<15} {metrics['map']:<10.4f} \"\n",
    "                     f\"{metrics['mrr'][10]:<10.4f} \"\n",
    "                     f\"{metrics['recall'][10]:<12.4f} \"\n",
    "                     f\"{metrics['ndcg'][10]:<10.4f} \"\n",
    "                     f\"{metrics.get('queries_per_second', 0):<10.2f} \"\n",
    "                     f\"{metrics.get('time', 0):<10.1f}\")\n",
    "\n",
    "       print(\"\\n\" + \"=\"*120)\n",
    "\n",
    "       # Additional insights\n",
    "       if 'hybrid' in results and 'dense' in results and 'bm25' in results:\n",
    "           hybrid_map = results['hybrid'].get('map', 0.0)\n",
    "           dense_map = results['dense'].get('map', 0.0)\n",
    "           bm25_map = results['bm25'].get('map', 0.0)\n",
    "\n",
    "           print(\"PERFORMANCE INSIGHTS:\")\n",
    "           if dense_map > 0:\n",
    "               improvement = ((hybrid_map - dense_map) / dense_map * 100)\n",
    "               print(f\"Hybrid vs Dense MAP improvement: {improvement:+.1f}%\")\n",
    "           if bm25_map > 0:\n",
    "               improvement = ((hybrid_map - bm25_map) / bm25_map * 100)\n",
    "               print(f\"Hybrid vs BM25 MAP improvement: {improvement:+.1f}%\")\n",
    "\n",
    "           # Find best method\n",
    "           best_method = max(methods, key=lambda m: results[m].get('map', -1) if m in results else -1)\n",
    "           if best_method in results:\n",
    "               print(f\"Best performing method: {best_method.upper()} (MAP: {results[best_method]['map']:.4f})\")\n",
    "\n",
    "           print(\"\\nHybrid Search Configuration:\")\n",
    "           print(f\"- Dense candidates: 500\")\n",
    "           print(f\"- BM25 re-ranking weight: 0.5\")\n",
    "           print(f\"- Final results returned: 100\")\n",
    "\n",
    "       print(\"=\"*120)\n",
    "\n",
    "def main():\n",
    "   \"\"\"Main function for full MS MARCO evaluation\"\"\"\n",
    "   logger.info(\"=\"*80)\n",
    "   logger.info(\"STARTING FULL MS MARCO EVALUATION SYSTEM\")\n",
    "   logger.info(\"This will download and process the COMPLETE MS MARCO dataset\")\n",
    "   logger.info(\"=\"*80)\n",
    "\n",
    "   try:\n",
    "       # Create evaluator\n",
    "       evaluator = MSMarcoEvaluator(\n",
    "           model_name=\"all-MiniLM-L6-v2\",\n",
    "           save_dir=\"./msmarco_full_eval\"\n",
    "       )\n",
    "\n",
    "       # Step 1: Load the complete dataset\n",
    "       logger.info(\"STEP 1: Loading complete MS MARCO dataset...\")\n",
    "       success = evaluator.load_full_msmarco_data()\n",
    "       if not success:\n",
    "           logger.error(\"Failed to load MS MARCO dataset\")\n",
    "           return\n",
    "\n",
    "       logger.info(\"Dataset loaded successfully!\")\n",
    "       logger.info(f\"- Documents: {len(evaluator.documents):,}\")\n",
    "       logger.info(f\"- Queries: {len(evaluator.dev_queries):,}\")\n",
    "       logger.info(f\"- QRels: {len(evaluator.qrels):,}\")\n",
    "\n",
    "       # Step 2: Build dense retrieval index with IVF\n",
    "       logger.info(\"STEP 2: Building dense retrieval index with IVF centroids...\")\n",
    "       evaluator.build_dense_index(batch_size=64)\n",
    "       logger.info(\"Dense index with IVF built successfully!\")\n",
    "\n",
    "       # Step 3: Build BM25 index\n",
    "       logger.info(\"STEP 3: Building BM25 sparse retrieval index...\")\n",
    "       evaluator.build_bm25_index()\n",
    "       logger.info(\"BM25 index built successfully!\")\n",
    "\n",
    "       # Step 4: Test search methods\n",
    "       logger.info(\"STEP 4: Testing search methods...\")\n",
    "       test_query = \"what is machine learning algorithm\"\n",
    "\n",
    "       # Test dense search\n",
    "       logger.info(\"Testing dense search...\")\n",
    "       dense_results = evaluator.search_dense(test_query, k=10)\n",
    "       logger.info(f\"Dense search: Retrieved {len(dense_results)} results\")\n",
    "       if dense_results:\n",
    "           logger.info(f\"Top result score: {dense_results[0][1]:.4f}\")\n",
    "\n",
    "       # Test BM25 search\n",
    "       logger.info(\"Testing BM25 search...\")\n",
    "       bm25_results = evaluator.search_bm25(test_query, k=10)\n",
    "       logger.info(f\"BM25 search: Retrieved {len(bm25_results)} results\")\n",
    "       if bm25_results:\n",
    "           logger.info(f\"Top result score: {bm25_results[0][1]:.4f}\")\n",
    "\n",
    "       # Test hybrid search (500 candidates from dense + BM25 reranking)\n",
    "       logger.info(\"Testing hybrid search (500 dense candidates + BM25 reranking)...\")\n",
    "       hybrid_results = evaluator.search_hybrid(test_query, k=10, candidate_k=500, alpha=0.5)\n",
    "       logger.info(f\"Hybrid search: Retrieved {len(hybrid_results)} results\")\n",
    "       if hybrid_results:\n",
    "           logger.info(f\"Top result score: {hybrid_results[0][1]:.4f}\")\n",
    "\n",
    "       # Step 5: Evaluation on dev set\n",
    "       logger.info(\"STEP 5: Starting evaluation on MS MARCO dev set...\")\n",
    "       \n",
    "       # Start with subset for quick test\n",
    "       logger.info(\"Running evaluation on 1000 queries for initial testing...\")\n",
    "       results = evaluator.evaluate_on_dev_set(max_queries=1000, sample_queries=True)\n",
    "\n",
    "       # Print results\n",
    "       evaluator.print_evaluation_results(results)\n",
    "\n",
    "       # Save initial results\n",
    "       results_path = os.path.join(evaluator.save_dir, 'evaluation_results_1000.json')\n",
    "       with open(results_path, 'w') as f:\n",
    "           json.dump(results, f, indent=2)\n",
    "       logger.info(f\"Results saved to {results_path}\")\n",
    "\n",
    "       # Option for full evaluation\n",
    "       print(\"\\nDo you want to run full evaluation on ALL dev queries? (y/n): \", end=\"\")\n",
    "       try:\n",
    "           response = input().lower().strip()\n",
    "           if response == 'y':\n",
    "               logger.info(\"Starting FULL evaluation on all dev queries...\")\n",
    "               logger.info(\"This may take several hours depending on your hardware...\")\n",
    "               \n",
    "               full_results = evaluator.evaluate_on_dev_set(max_queries=None)\n",
    "               evaluator.print_evaluation_results(full_results)\n",
    "               \n",
    "               # Save full results\n",
    "               full_results_path = os.path.join(evaluator.save_dir, 'full_evaluation_results.json')\n",
    "               with open(full_results_path, 'w') as f:\n",
    "                   json.dump(full_results, f, indent=2)\n",
    "               logger.info(f\"Full results saved to {full_results_path}\")\n",
    "               \n",
    "       except KeyboardInterrupt:\n",
    "           logger.info(\"Evaluation interrupted by user\")\n",
    "\n",
    "       logger.info(\"=\"*80)\n",
    "       logger.info(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "       logger.info(\"All data has been cached for future runs\")\n",
    "       logger.info(\"=\"*80)\n",
    "\n",
    "   except KeyboardInterrupt:\n",
    "       logger.info(\"Evaluation interrupted by user\")\n",
    "   except Exception as e:\n",
    "       logger.error(f\"Critical error in main execution: {e}\", exc_info=True)\n",
    "       raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
